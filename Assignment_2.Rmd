---
title: "MSB1015 Scientific Programming: Assignment 2"
author: "Lian Laudy"
date: "5 october 2018"
output: html_document
---

# Packages

Check if the required packages are already installed. If not, install them:
```{r}
packages = c("matrixStats", "Boruta", "pls")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}
```

Load the required packages:
```{r}
# the package 'matrixStats' is used to calculate the variance across each column (descriptor)
library(matrixStats)
# the package 'Boruta' is used to perform feature selection based on a Random Forest method
library(Boruta)
# the package 'pls' is used to perfrom the Partial Least Squares analysis
library(pls)
```

# Load data

Load the data files.
The data file called 'score' has to contain the 'SID' in the first column and the Activity Score in the second column.
The data file called 'descriptors' has to contain the 'SID' in the first column and the descriptors in the remaining columns. 
```{r}
score = read.csv(file = 'C:/Users/Lian/Desktop/CloudStation/MSB1015 Scientific Programming/Assignments/2 Making multivariate statistics reproducible/qsarSmallData.csv', sep=',', row.names = 1, header=TRUE)

descriptors = read.csv(file = 'C:/Users/Lian/Desktop/CloudStation/MSB1015 Scientific Programming/Assignments/2 Making multivariate statistics reproducible/descriptors.csv', sep=',', row.names = 1, header=TRUE)
```

Sort both dataframes by substance ID such that the order of molecules matches:
```{r}
score = score[order(score$SID),]
row.names(score) = c(1:nrow(score))
descriptors =  descriptors[order(descriptors$molSIDs),]
row.names(descriptors) = c(1:nrow(descriptors))

#check if sorting went well:
table(score[,1] == descriptors[,1])
```
# Split data

Create a training set (80%) and a test set (20%):  
```{r}
# use set.seed() to ensure that the same random numbers are used and the results are reproducible
set.seed(0)
# selected 80% of the total number of rows
selection = sample(nrow(descriptors), 0.8*nrow(descriptors))
# use this selection to select the training data
score_tr = score[selection, ]
descs_tr = descriptors[selection, ]
# the same selection will be excluded from the test data
score_te = score[-selection, ]
descs_te = descriptors[-selection, ]
```

# Data cleaning

The training data contains many Na values:
```{r}
table(is.na(descs_tr))
table(colSums(is.na(descs_tr)))
```

Remove descriptors (columns) that contain at least one missing value:
```{r}
# we only include the columns in which the number of row that do NOT contain missing values is equal to the total number of rows
descs_tr2 = descs_tr[,colSums(!is.na(descs_tr)) == nrow(descs_tr)]
descs_te = descs_te[,colSums(!is.na(descs_tr)) == nrow(descs_tr)]
descs_tr = descs_tr2
remove(descs_tr2)
```

# Feature selection (based on variance)
In order to reduce the number of descriptors, we selected descriptors based on their variance.
Only descriptors with a variance higher than the third quantile were selected.

```{r}
# Compute for each descriptor the variance (except column 1 which contains the molecule ID):
variance_tr = colVars(as.matrix(descs_tr[,2:length(descs_tr)]))
summary(variance_tr)

# Calculate the value of the third quantile of the variance:
quantile75 = quantile(variance_tr, .75)

# Select only the descriptors with a variance higher than the third quantile (plus the first column containing the molecule ID):
descs_tr = descs_tr[,c(TRUE, (variance_tr > quantile75))]
descs_te = descs_te[,c(TRUE, (variance_tr > quantile75))]

# Number of removed descriptors based on variance:
(length(variance_tr)+1)-ncol(descs_tr)

# Number of included descriptors (excluding the molecule ID):
ncol(descs_tr)-1
```

# Data transformation
The data is scaled in order to standardize the range of the descriptors.  
```{r}
# Calculate the mean of each descriptor in the training set:
mean_descriptor = colMeans(descs_tr[,2:length(descs_tr)])
  
# Calculate the standard deviation of each descriptor in the training set:
sd_descriptor = apply(as.matrix(descs_tr[,2:length(descs_tr)]), 2, sd)
  
# Generate data frames for the normalized training and test set
descs_tr_norm = as.data.frame(array(data = NA, dim = c(nrow(descs_tr),ncol(descs_tr))))
descs_tr_norm[,1] =  descs_tr[,1]
colnames(descs_tr_norm) = colnames(descs_tr)

descs_te_norm = as.data.frame(array(data = NA, dim = c(nrow(descs_te),ncol(descs_te))))
descs_te_norm[,1] =  descs_te[,1]
colnames(descs_te_norm) = colnames(descs_te)

# Normalize each descriptor in the training and test set, both using the mean and standard deviation of the training set as you otherwise would already use information from the test set.
for (j in 2:ncol(descs_tr_norm)) { 
    descs_tr_norm[,j] = (descs_tr[,j] - mean_descriptor[j-1]) / sd_descriptor[j-1]
    descs_te_norm[,j] = (descs_te[,j] - mean_descriptor[j-1]) / sd_descriptor[j-1]
}
```

# Feature selection (based on Random Forest)
In order to reduce the number of descriptors even more, we use a feature selection method that is based on Random Forest.
Only descriptors that are considered to have a confirmed importance will be selected.
```{r}
# Add Activity score to corresponding molecule:
descs_tr_score = cbind(descs_tr_norm, score[match(descs_tr_norm[,1], score[,1]),2])
colnames(descs_tr_score)[length(descs_tr_score)] = 'Score'
descs_te_score = cbind(descs_te_norm, score[match(descs_te_norm[,1], score[,1]),2])
colnames(descs_te_score)[length(descs_te_score)] = 'Score'
  
# Run Boruta algorithm:
set.seed(0)
boruta_descs_tr = Boruta(Score~.,data=descs_tr_score[,2:length(descs_tr_score)],doTrace=1)
  
# Select descriptors that turned out to be important:
print(boruta_descs_tr)
boruta_logical = boruta_descs_tr$finalDecision == "Confirmed"

# Select data of the important descriptors (plus molecule ID and score) in the training and test set
descs_tr_select = descs_tr_score[, c(TRUE, (boruta_logical), TRUE)]
descs_te_select = descs_te_score[, c(TRUE, (boruta_logical), TRUE)]
```

# Partial Least Squares
The partial least squares method is used to build a model based on the training set and, consequently, this trained model is used to predict the activity score of the test set. Finally, the predicted and observed scores are compared with each other. 
```{r}
# Generate the correct formulation of input (descriptors) necessary for PLS function
names(descs_tr_select) = make.names(names(descs_tr_select))
name_descs_tr_select = names(descs_tr_select)
formula_descs_tr_select = as.formula(paste("Score ~", paste(name_descs_tr_select[!name_descs_tr_select %in% c("molSIDs","Score")], collapse = " + ")))
  
# Perform a PLS analysis with cross-validation and a maximum of 6 components 
pls_result = plsr(formula_descs_tr_select, 6, data = descs_tr_select[, 2:length(descs_tr_select)], validation = "CV")

# Plot the RMSEP for each number of components included in the model
plot(RMSEP(pls_result))

# The RMSEP turns out to be the lowest if 2 components are included in the model. Therefore, the final model will include 2 components
final_model = predict(pls_result, ncomp = 2, newdata = descs_te_select)

# Plot the observed score versus the predicted score
predplot(pls_result, ncomp = 2, newdata = descs_te_select)
```
